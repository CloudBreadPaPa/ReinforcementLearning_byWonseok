{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C, Tensorflow 로 구현\n",
    "\n",
    "Reinforcement learning의 학습 알고리즘 중 하나인 A3C를 Tensorflow로 구현하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/i18k6m0rrtp7wem/Screenshot%202018-07-04%2000.44.32.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import multiprocessing\n",
    "import shutil\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 에이전트의 수\n",
    "no_of_workers = multiprocessing.cpu_count() \n",
    "\n",
    "print(no_of_workers)\n",
    "\n",
    "# episode의 time steps \n",
    "\n",
    "no_of_ep_steps = 200 \n",
    "\n",
    "# 총 episode \n",
    "\n",
    "no_of_episodes = 2000 \n",
    "\n",
    "global_net_scope = 'Global_Net'\n",
    "\n",
    "# global network의 update 빈도수 설정 \n",
    "\n",
    "update_global = 100 \n",
    "\n",
    "# discount factor \n",
    "\n",
    "gamma = 0.99 \n",
    "\n",
    "# entropy factor \n",
    "\n",
    "entropy_beta = 0.01 \n",
    "\n",
    "#actor의 learning rate \n",
    "\n",
    "lr_a = 0.0001\n",
    "\n",
    "#critic의 learning rate \n",
    "\n",
    "lr_c=0.001 \n",
    "\n",
    "# environment render 유,무 \n",
    "\n",
    "render = True \n",
    "\n",
    "\n",
    "# 저장 디렉토리 \n",
    "\n",
    "log_dir = 'logs '\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 환경 import \n",
    "\n",
    "![download](https://user-images.githubusercontent.com/11300712/42167947-891206e2-7e4a-11e8-8918-4c09db5d6c1d.png)\n",
    "\n",
    "openai gym에서 MountainCar를 불러온다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-09 08:57:19,691] Making new env: MountainCarContinuous-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.56173612,  0.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env= gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# 환경 초기화 \n",
    "\n",
    "env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# state, action, bound 받기\n",
    "no_of_states = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.shape[0]\n",
    "action_bound = [env.action_space.low, env.action_space.high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "[array([-1.]), array([ 1.])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#state의 수\n",
    "\n",
    "number_state = env.observation_space.shape[0]\n",
    "\n",
    "print(number_state)\n",
    "\n",
    "# action의 수 \n",
    "\n",
    "number_actions = env.action_space.shape[0]\n",
    "\n",
    "print(number_actions)\n",
    "\n",
    "# action의 bound \n",
    "\n",
    "\n",
    "action_bound = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "print(action_bound)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Actor Crtic network 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/pwed476ggbzwqxz/Screenshot%202018-07-03%2022.32.59.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCritic(object):\n",
    "     def __init__(self, scope, sess, globalAC=None):\n",
    "         \n",
    "        #actor과 critic 네트워크의 session과 RMS prop optimizer을 initializer한다. \n",
    "      \n",
    "        \n",
    "        self.sess=sess\n",
    "        \n",
    "        self.actor_optimizer = tf.train.RMSPropOptimizer(lr_a, name='RMSPropA')\n",
    "        self.critic_optimizer = tf.train.RMSPropOptimizer(lr_c, name='RMSPropC')\n",
    " \n",
    "        #네트워크가 글로벌일때\n",
    "        if scope == global_net_scope:\n",
    "            with tf.variable_scope(scope):\n",
    "                # actor과 critic 네트워크를 build 하고 state를 초기화 한다. \n",
    "               \n",
    "                self.s = tf.placeholder(tf.float32, [None, no_of_states], 'S')\n",
    "                \n",
    "                # actor과 critic 네트워크의 파라\n",
    "                # get the parameters of actor and critic networks\n",
    "                self.a_params, self.c_params = self._build_net(scope)[-2:]\n",
    "                \n",
    "        # 네트워크가 로컬일때\n",
    "        else:\n",
    "            with tf.variable_scope(scope):\n",
    "                \n",
    "                # state를 초기화하고 action과 target value를 v_target으로 \n",
    "                # initialize state, action and also target value as v_target\n",
    "                \n",
    "                self.s = tf.placeholder(tf.float32, [None, no_of_states], 'S')\n",
    "                self.a_his = tf.placeholder(tf.float32, [None, no_of_actions], 'A')\n",
    "                self.v_target = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
    "                \n",
    "                # continuous actions space이기때문에 선택된 action의 mean과 variance를 구한다. \n",
    "               \n",
    "                \n",
    "                mean, var, self.v, self.a_params, self.c_params = self._build_net(scope)\n",
    "                # td error계산 \n",
    "                \n",
    "                td = tf.subtract(self.v_target, self.v, name='TD_error')\n",
    "                # td error을 minimize 한다. \n",
    "                \n",
    "            \n",
    "                with tf.name_scope('critic_loss'):\n",
    "                    self.critic_loss = tf.reduce_mean(tf.square(td))\n",
    "\n",
    "               \n",
    "                # mean과 var value를 update한다. \n",
    "              \n",
    "                with tf.name_scope('wrap_action'):\n",
    "                    mean, var = mean * action_bound[1], var + 1e-4\n",
    "                                            \n",
    "                #mean과 var의 업데이트를 사용하여 distribution을 generate한다. \n",
    "                normal_dist = tf.contrib.distributions.Normal(mean, var)\n",
    "                \n",
    "                \n",
    "                \n",
    "               \n",
    "                with tf.name_scope('actor_loss'):\n",
    "                    # loss , log(pi(s))를 계산한다. \n",
    "                    \n",
    "                    log_prob = normal_dist.log_prob(self.a_his)\n",
    "                    exp_v = log_prob * td\n",
    "                    \n",
    "                    # exploration을 위해 action distribution의 entropy를 계산한다. \n",
    "                    \n",
    "                    entropy = normal_dist.entropy()\n",
    "                    \n",
    "                    # final loss를 다음과 같이 계산한다. \n",
    "                  \n",
    "                    self.exp_v = exp_v + entropy_beta * entropy\n",
    "                    \n",
    "                    # loss를 minimize한다. \n",
    "                    self.actor_loss = tf.reduce_mean(-self.exp_v)\n",
    "                   \n",
    "                # distribution에 의해 action을 선택하고 action의 bound로 clipping한다. \n",
    "                \n",
    "                with tf.name_scope('choose_action'):\n",
    "                    self.A = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0), action_bound[0], action_bound[1])\n",
    "                 # actor와  critic 네트워크의 gradients를 계산한다. \n",
    "               \n",
    "                with tf.name_scope('local_grad'):\n",
    "\n",
    "                    self.a_grads = tf.gradients(self.actor_loss, self.a_params)\n",
    "                    self.c_grads = tf.gradients(self.critic_loss, self.c_params)\n",
    "             \n",
    "            # 글로벌 네트워크의 웨이트를 업데이트한다. \n",
    "    \n",
    "            with tf.name_scope('sync'):\n",
    "                \n",
    "                # 글로벌 네트워크 웨이트를 로컬 네트워크로 pull한다. \n",
    "               \n",
    "                with tf.name_scope('pull'):\n",
    "                    self.pull_a_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.a_params, globalAC.a_params)]\n",
    "                    self.pull_c_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.c_params, globalAC.c_params)]\n",
    "                \n",
    "                #local gradients를 global network로 push 한다. \n",
    "              \n",
    "                with tf.name_scope('push'):\n",
    "                    self.update_a_op = self.actor_optimizer.apply_gradients(zip(self.a_grads, globalAC.a_params))\n",
    "                    self.update_c_op = self.critic_optimizer.apply_gradients(zip(self.c_grads, globalAC.c_params))\n",
    "                    \n",
    "        \n",
    "\n",
    "     # actor와 critic 네트워크를 build하기 위해 함수를 정의한다. \n",
    "    \n",
    "     def _build_net(self, scope):\n",
    "     # 웨이트를 초기화한다. \n",
    "        w_init = tf.random_normal_initializer(0., .1)\n",
    "        \n",
    "        with tf.variable_scope('actor'):\n",
    "            l_a = tf.layers.dense(self.s, 200, tf.nn.relu6, kernel_initializer=w_init, name='la')\n",
    "            mean = tf.layers.dense(l_a, no_of_actions, tf.nn.tanh,kernel_initializer=w_init, name='mean')\n",
    "            var = tf.layers.dense(l_a, no_of_actions, tf.nn.softplus, kernel_initializer=w_init, name='var')\n",
    "            \n",
    "        with tf.variable_scope('critic'):\n",
    "            l_c = tf.layers.dense(self.s, 100, tf.nn.relu6, kernel_initializer=w_init, name='lc')\n",
    "            v = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='v')\n",
    "        \n",
    "        a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
    "        c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
    "        \n",
    "        return mean, var, v, a_params, c_params\n",
    "    \n",
    "         \n",
    "     # 글로벌네트워크로 로컬 gradient 를 업데이트 한다. \n",
    "     def update_global(self, feed_dict):\n",
    "        self.sess.run([self.update_a_op, self.update_c_op], feed_dict)\n",
    "     \n",
    "     \n",
    "     def pull_global(self):\n",
    "        self.sess.run([self.pull_a_params_op, self.pull_c_params_op])\n",
    "     \n",
    "     # action을 선택\n",
    "     def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.A, {self.s: s})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    def __init__(self, name, globalAC, sess):\n",
    "        #각 환경의 worker을 초기화 시킨다. \n",
    "        self.env= gym.make('MountainCarContinuous-v0').unwrapped\n",
    "        self.name = name\n",
    "        \n",
    "        \n",
    "        #ActorCritic 에이전트를 만든다 \n",
    "        self.AC = ActorCritic(name,sess, globalAC)\n",
    "        self.sess =sess\n",
    "        \n",
    "    def work(self):\n",
    "        global global_rewards, global_episodes\n",
    "        \n",
    "        total_step = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #state, action, reward를 저장한다.\n",
    "        buffer_s, buffer_a, buffer_r= [],[],[]\n",
    "        \n",
    "        \n",
    "        # loop if the coordinator is active and global episode is less than the maximum episode\n",
    "        \n",
    "        while not coord.should_stop() and global_episodes < no_of_episodes:\n",
    "            # 리셋을 하여 환경을 초기화 한다. \n",
    "            \n",
    "            s= self.env.reset()\n",
    "            \n",
    "            #episode의 reward를 store한다. \n",
    "            \n",
    "            ep_r = 0\n",
    "            \n",
    "            for ep_t in range(no_of_ep_steps):\n",
    "                \n",
    "                # worker 1의 환경을 render한다. \n",
    "               \n",
    "                \n",
    "                if self.name=='W_0' and render: \n",
    "                    \n",
    "                    self.env.render()\n",
    "                    \n",
    "                    \n",
    "                    #policy를 이용하여 action을 선택한다. \n",
    "                    \n",
    "                a = self.AC.choose_action(s)\n",
    "                    \n",
    "                    #action a를 선택하고, reward를 받으며, next state s_t+1로 움직인다. \n",
    "                    \n",
    "                s_,r,done,info =self.env.step(a)\n",
    "                    \n",
    "                    #set done as true if we reached maximum step per episode\n",
    "                    \n",
    "                done = True if ep_t == no_of_ep_steps -1 else False\n",
    "                    \n",
    "                ep_r += r \n",
    "                    \n",
    "                    #state, action, reward를 buffer에 넣는다. \n",
    "                    \n",
    "                buffer_s.append(s)\n",
    "                buffer_a.append(a)\n",
    "                    \n",
    "                    #reward를 normalize한다. \n",
    "                    \n",
    "                buffer_r.append((r+8)/8)\n",
    "                #일정한 타임스텝 뒤에 글로벌 네트워크를 업데이트 한다. \n",
    "                    \n",
    "                if total_step % update_global == 0 or done:\n",
    "                    if done:\n",
    "                        v_s_= 0\n",
    "                    else:\n",
    "                        v_s_=self.sess.run(self.AC.v, {self.AC.s: s_[np.newaxis,:]})[0,0]\n",
    "                            \n",
    "                        # target v 를 위한 buffer\n",
    "                        \n",
    "                    buffer_v_target = []\n",
    "                        \n",
    "                    for r in buffer_r[::-1]:\n",
    "                        v_s_= r+gamma * v_s_\n",
    "                        buffer_v_target.append(v_s_)\n",
    "                            \n",
    "                    buffer_v_target.reverse()\n",
    "                        \n",
    "                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.vstack(buffer_a), np.vstack(buffer_v_target)    \n",
    "                    feed_dict = {\n",
    "                        self.AC.s : buffer_s,\n",
    "                        self.AC.a_his : buffer_a,\n",
    "                        self.AC.v_target:buffer_v_target,\n",
    "                    }\n",
    "        \n",
    "                        #글로벌 네트워크 업데이트 \n",
    "            \n",
    "                    self.AC.update_global(feed_dict)\n",
    "                    buffer_s, buffer_a, buffer_r =[],[],[]\n",
    "                    \n",
    "                        # get global parameters to local ActorCritic \n",
    "                    self.AC.pull_global()\n",
    "                        \n",
    "                s=s\n",
    "                total_step += 1\n",
    "                    \n",
    "                if done : \n",
    "                    if len(global_rewads) < 5:\n",
    "                        global_rewards.append(ep_r)\n",
    "                    else: \n",
    "                        global_rewards.append(ep_r)\n",
    "                        global_rewards[-1]=(np.mean(global_rewards[-5:1]))\n",
    "                            \n",
    "                            \n",
    "                    global_episodes += 1\n",
    "                    \n",
    "                break \n",
    "                        \n",
    "          \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list for string global rewards and episodes\n",
    "\n",
    "global_rewards = []\n",
    "global_episodes = 0\n",
    "\n",
    "#텐서플로우 시작 \n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    \n",
    "    #ActorCritic 클래스의 instance를 만든다. \n",
    "    global_ac = ActorCritic(global_net_scope,sess)\n",
    "    \n",
    "    workers = []\n",
    "    \n",
    "    # loop for each workers\n",
    "    for i in range(no_of_workers):\n",
    "        i_name = 'W_%i' % i\n",
    "        workers.append(Worker(i_name, global_ac,sess))\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#모든것을 log로, 텐서보드로 visualize하기 위해 \n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "worker_threads = []\n",
    "\n",
    "\n",
    "\n",
    "for worker in workers:\n",
    "    \n",
    "    job = lambda: wormbker.work()\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
